{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE176 Assignment 3: Neural Network in NumPy\n",
    "\n",
    "Use this notebook to build your neural network by implementing the following functions in the python files under `layers` directory:\n",
    "\n",
    "1. `linear.py`\n",
    "2. `relu.py`\n",
    "3. `softmax.py`\n",
    "4. `loss_func.py`\n",
    "\n",
    "You will be testing your 2 layer neural network implementation on a toy dataset.\n",
    "\n",
    "TO SUBMIT: PDF of this notebook with all the required outputs and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from layers.sequential import Sequential\n",
    "from layers.linear import Linear\n",
    "from layers.relu import ReLU\n",
    "from layers.softmax import Softmax\n",
    "from layers.loss_func import CrossEntropyLoss\n",
    "from utils.optimizer import SGD\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # set default size of plots\n",
    "\n",
    "# For auto-reloading external modules\n",
    "# See http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the class Sequential as implemented in the file layers/sequential.py to build a layer by layer model of our neural network. Below we initialize the toy model and the toy random data that you will use to develop your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small net and some toy data to check your implementations.\n",
    "# Note that we set the random seed for repeatable experiments.\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3  # Output\n",
    "num_inputs = 10  # N\n",
    "\n",
    "\n",
    "def init_toy_model():\n",
    "    np.random.seed(0)\n",
    "    l1 = Linear(input_size, hidden_size)\n",
    "    l2 = Linear(hidden_size, num_classes)\n",
    "\n",
    "    r1 = ReLU()\n",
    "    softmax = Softmax()\n",
    "    return Sequential([l1, r1, l2, softmax])\n",
    "\n",
    "\n",
    "def init_toy_data():\n",
    "    np.random.seed(0)\n",
    "    X = 10 * np.random.randn(num_inputs, input_size)\n",
    "    y = np.random.randint(num_classes, size=num_inputs)\n",
    "    # y = np.array([0, 1, 2, 2, 1])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "net = init_toy_model()\n",
    "X, y = init_toy_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass: Compute Scores (20%)\n",
    "Implement the forward functions in Linear, Relu and Softmax layers and get the output by passing our toy data X\n",
    "The output must match the given output scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your scores:\n",
      "[[0.33333514 0.33333826 0.33332661]\n",
      " [0.3333351  0.33333828 0.33332661]\n",
      " [0.3333351  0.33333828 0.33332662]\n",
      " [0.3333351  0.33333828 0.33332662]\n",
      " [0.33333509 0.33333829 0.33332662]\n",
      " [0.33333508 0.33333829 0.33332662]\n",
      " [0.33333511 0.33333828 0.33332661]\n",
      " [0.33333512 0.33333827 0.33332661]\n",
      " [0.33333508 0.33333829 0.33332662]\n",
      " [0.33333511 0.33333828 0.33332662]]\n",
      "\n",
      "correct scores:\n",
      "[[0.33333514 0.33333826 0.33332661]\n",
      " [0.3333351  0.33333828 0.33332661]\n",
      " [0.3333351  0.33333828 0.33332662]\n",
      " [0.3333351  0.33333828 0.33332662]\n",
      " [0.33333509 0.33333829 0.33332662]\n",
      " [0.33333508 0.33333829 0.33332662]\n",
      " [0.33333511 0.33333828 0.33332661]\n",
      " [0.33333512 0.33333827 0.33332661]\n",
      " [0.33333508 0.33333829 0.33332662]\n",
      " [0.33333511 0.33333828 0.33332662]]\n",
      "Difference between your scores and correct scores:\n",
      "8.799388540037256e-08\n"
     ]
    }
   ],
   "source": [
    "scores = net.forward(X)\n",
    "print(\"Your scores:\")\n",
    "print(scores)\n",
    "print()\n",
    "print(\"correct scores:\")\n",
    "correct_scores = np.asarray(\n",
    "    [\n",
    "        [0.33333514, 0.33333826, 0.33332661],\n",
    "        [0.3333351, 0.33333828, 0.33332661],\n",
    "        [0.3333351, 0.33333828, 0.33332662],\n",
    "        [0.3333351, 0.33333828, 0.33332662],\n",
    "        [0.33333509, 0.33333829, 0.33332662],\n",
    "        [0.33333508, 0.33333829, 0.33332662],\n",
    "        [0.33333511, 0.33333828, 0.33332661],\n",
    "        [0.33333512, 0.33333827, 0.33332661],\n",
    "        [0.33333508, 0.33333829, 0.33332662],\n",
    "        [0.33333511, 0.33333828, 0.33332662],\n",
    "    ]\n",
    ")\n",
    "print(correct_scores)\n",
    "\n",
    "# The difference should be very small. We get < 1e-7\n",
    "print(\"Difference between your scores and correct scores:\")\n",
    "print(np.sum(np.abs(scores - correct_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass: Compute loss given the output scores from the previous step (10%)\n",
    "Implement the forward function in the loss_func.py file, and output the loss value. The loss value must match the given loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0986124335483813\n",
      "Difference between your loss and correct loss:\n",
      "2.8981419664120267e-07\n"
     ]
    }
   ],
   "source": [
    "Loss = CrossEntropyLoss()\n",
    "loss = Loss.forward(scores, y)\n",
    "correct_loss = 1.098612723362578\n",
    "print(loss)\n",
    "# should be very small, we get < 1e-12\n",
    "print(\"Difference between your loss and correct loss:\")\n",
    "print(np.sum(np.abs(loss - correct_loss)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass (40%)\n",
    "Implement the rest of the functions in the given files. Specifically, implement the backward function in all the 4 files as mentioned in the files. Note: No backward function in the softmax file, the gradient for softmax is jointly calculated with the cross entropy loss in the loss_func.backward function.\n",
    "\n",
    "You will use the chain rule to calculate gradient individually for each layer. You can assume that this calculated gradeint then is passed to the next layers in a reversed manner due to the Sequential implementation. So all you need to worry about is implementing the gradient for the current layer and multiply it will the incoming gradient (passed to the backward function as dout) to calculate the total gradient for the parameters of that layer.\n",
    "\n",
    "We check the values for these gradients by calculating the difference, it is expected to get difference < 1e-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 10)\n",
      "(10,)\n",
      "(10, 3)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "# No need to edit anything in this block ( 20% of the above 40% )\n",
    "net.backward(Loss.backward())\n",
    "\n",
    "gradients = []\n",
    "for module in net._modules:\n",
    "    for para, grad in zip(module.parameters, module.grads):\n",
    "        assert grad is not None, \"No Gradient\"\n",
    "        # Print gradients of the linear layer\n",
    "        print(grad.shape)\n",
    "        gradients.append(grad)\n",
    "\n",
    "# Check shapes of your gradient. Note that only the linear layer has parameters\n",
    "# (4, 10) -> Layer 1 W\n",
    "# (10,)   -> Layer 1 b\n",
    "# (10, 3) -> Layer 2 W\n",
    "# (3,)    -> Layer 2 b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in Gradient values 7.70191643436727e-09\n"
     ]
    }
   ],
   "source": [
    "# No need to edit anything in this block ( 20% of the above 40% )\n",
    "grad_w1 = np.array(\n",
    "    [\n",
    "        [\n",
    "            -6.24320917e-05,\n",
    "            3.41037180e-06,\n",
    "            -1.69125969e-05,\n",
    "            2.41514079e-05,\n",
    "            3.88697976e-06,\n",
    "            7.63842314e-05,\n",
    "            -8.88925758e-05,\n",
    "            3.34909890e-05,\n",
    "            -1.42758303e-05,\n",
    "            -4.74748560e-06,\n",
    "        ],\n",
    "        [\n",
    "            -7.16182867e-05,\n",
    "            4.63270039e-06,\n",
    "            -2.20344270e-05,\n",
    "            -2.72027034e-06,\n",
    "            6.52903437e-07,\n",
    "            8.97294847e-05,\n",
    "            -1.05981609e-04,\n",
    "            4.15825391e-05,\n",
    "            -2.12210745e-05,\n",
    "            3.06061658e-05,\n",
    "        ],\n",
    "        [\n",
    "            -1.69074923e-05,\n",
    "            -8.83185056e-06,\n",
    "            3.10730840e-05,\n",
    "            1.23010428e-05,\n",
    "            5.25830316e-05,\n",
    "            -7.82980115e-06,\n",
    "            3.02117990e-05,\n",
    "            -3.37645284e-05,\n",
    "            6.17276346e-05,\n",
    "            -1.10735656e-05,\n",
    "        ],\n",
    "        [\n",
    "            -4.35902272e-05,\n",
    "            3.71512704e-06,\n",
    "            -1.66837877e-05,\n",
    "            2.54069557e-06,\n",
    "            -4.33258099e-06,\n",
    "            5.72310022e-05,\n",
    "            -6.94881762e-05,\n",
    "            2.92408329e-05,\n",
    "            -1.89369767e-05,\n",
    "            2.01692516e-05,\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "grad_b1 = np.array(\n",
    "    [\n",
    "        -2.27150209e-06,\n",
    "        5.14674340e-07,\n",
    "        -2.04284403e-06,\n",
    "        6.08849787e-07,\n",
    "        -1.92177796e-06,\n",
    "        3.92085824e-06,\n",
    "        -5.40772636e-06,\n",
    "        2.93354593e-06,\n",
    "        -3.14568138e-06,\n",
    "        5.27501592e-11,\n",
    "    ]\n",
    ")\n",
    "\n",
    "grad_w2 = np.array(\n",
    "    [\n",
    "        [1.28932983e-04, 1.19946731e-04, -2.48879714e-04],\n",
    "        [1.08784150e-04, 1.55140199e-04, -2.63924349e-04],\n",
    "        [6.96017544e-05, 1.42748410e-04, -2.12350164e-04],\n",
    "        [9.92512487e-05, 1.73257611e-04, -2.72508860e-04],\n",
    "        [2.05484895e-05, 4.96161144e-05, -7.01646039e-05],\n",
    "        [8.20539510e-05, 9.37063861e-05, -1.75760337e-04],\n",
    "        [2.45831715e-05, 8.74369112e-05, -1.12020083e-04],\n",
    "        [1.34073379e-04, 1.86253064e-04, -3.20326443e-04],\n",
    "        [8.86473128e-05, 2.35554414e-04, -3.24201726e-04],\n",
    "        [3.57433149e-05, 1.91164061e-04, -2.26907376e-04],\n",
    "    ]\n",
    ")\n",
    "\n",
    "grad_b2 = np.array([-0.1666649, 0.13333828, 0.03332662])\n",
    "\n",
    "difference = (\n",
    "    np.sum(np.abs(gradients[0] - grad_w1))\n",
    "    + np.sum(np.abs(gradients[1] - grad_b1))\n",
    "    + np.sum(np.abs(gradients[2] - grad_w2))\n",
    "    + np.sum(np.abs(gradients[3] - grad_b2))\n",
    ")\n",
    "print(\"Difference in Gradient values\", difference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the complete network on the toy data. (29%)\n",
    "\n",
    "To train the network we will use stochastic gradient descent (SGD), we have implemented the optimizer for you. You do not implement any more functions in the python files. Below we implement the training procedure, you should get yourself familiar with the training process. Specifically looking at which functions to call and when.\n",
    "\n",
    "Once you have implemented the method and tested various parts in the above blocks, run the code below to train a two-layer network on toy data. You should see your training loss decrease below 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, loss=0.980217\n",
      "Epoch 100, loss=0.807396\n",
      "Epoch 150, loss=0.625599\n",
      "Epoch 200, loss=0.586722\n",
      "Epoch 250, loss=0.575466\n",
      "Epoch 300, loss=0.570104\n",
      "Epoch 350, loss=0.566874\n",
      "Epoch 400, loss=0.564617\n",
      "Epoch 450, loss=0.562804\n",
      "Epoch 500, loss=0.561491\n",
      "Epoch 550, loss=0.560438\n",
      "Epoch 600, loss=0.559628\n",
      "Epoch 650, loss=0.558960\n",
      "Epoch 700, loss=0.558381\n",
      "Epoch 750, loss=0.557910\n",
      "Epoch 800, loss=0.557495\n",
      "Epoch 850, loss=0.557134\n",
      "Epoch 900, loss=0.556807\n",
      "Epoch 950, loss=0.556522\n",
      "Epoch 1000, loss=0.556262\n"
     ]
    }
   ],
   "source": [
    "# Training Procedure\n",
    "# Initialize the optimizer. DO NOT change any of the hyper-parameters here or above.\n",
    "# We have implemented the SGD optimizer class for you here, which visits each layer sequentially to\n",
    "# get the gradients and optimize the respective parameters.\n",
    "# You should work with the given parameters and only edit your implementation in the .py files\n",
    "\n",
    "epochs = 1000\n",
    "optim = SGD(net, lr=0.1, weight_decay=0.00001)\n",
    "\n",
    "epoch_loss = []\n",
    "for epoch in range(epochs):\n",
    "    # Get output scores from the network\n",
    "    output_x = net(X)\n",
    "    # Calculate the loss for these output scores, given the true labels\n",
    "    loss = Loss.forward(output_x, y)\n",
    "    # Initialize your gradients to None in each epoch\n",
    "    optim.zero_grad()\n",
    "    # Make a backward pass to update the internal gradients in the layers\n",
    "    net.backward(Loss.backward())\n",
    "    # call the step function in the optimizer to update the values of the params with the gradients\n",
    "    optim.step()\n",
    "    # Append the loss at each iteration\n",
    "    epoch_loss.append(loss)\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(\"Epoch {}, loss={:3f}\".format(epoch + 1, epoch_loss[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 1 2 0 0 2 0 0]\n",
      "[2 1 0 1 2 0 0 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Test your predictions. The predictions must match the labels\n",
    "print(net.predict(X))\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training loss 0.5562624836499961\n"
     ]
    }
   ],
   "source": [
    "# You should be able to achieve a training loss of less than 0.02 (10%)\n",
    "print(\"Final training loss\", epoch_loss[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training loss curve. The loss in the curve should be decreasing (20%)\n",
    "plt.plot(epoch_loss)\n",
    "plt.title(\"Loss history\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey (1%)\n",
    "\n",
    "### Question:\n",
    "\n",
    "How many hours did you spend on this assignment?\n",
    "\n",
    "### Your Answer:\n",
    "\n",
    "I spent 20 hours on this assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
